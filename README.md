[Natural Language project to help in reducing Bias in ML](https://github.com/masdeval/jigsaw-unintended-bias-in-toxicity-classification/final_project.pdf)<p>
In the last several years it has become increasingly
obvious that machine learning (ML) and artificial intelligence (AI)
algorithms have systematic social bias built into the models. This
has been found to either mirror existing bias, or even in some
cases amplify it. As algorithms continue to play an increasingly
import role in our society, it is imperative that we come up
with techniques for both measuring fairness in ML, and also
techniques to remediate the identified fairness deficiencies.
A standard use case in ML and specifically natural language
processing (NLP) is the classification of textual content as either
”toxic” or ”non-toxic”. Toxic comments plague many web-based
community forums to the point where their utility is diminished.
There is often too many comments for human moderators to
manually evaluate for toxicity. Thus it is necessary that we have
machine-based algorithms for quickly and accurately identifying
and removing such comments.
It has recently been discovered that many state-of-art toxic
comment classifiers are highly biased against certain groups. For
example, online forums within the LGBTQ community may use
the word ”gay” frequently in a non-toxic way. However, state-of-
art toxic comment classifiers will likely label this content as toxic,
which disproportionately effects members of this community in
an adverse way. In this project, we will participate in a Kaggle
competition with the goal of building a fairness optimized
toxic comment classifier. We will evaluate and explore the dataset
provided by the competition and determine likely causes of bias,
and we will utilize techniques learned in the course of this class
to train a ML classification algorithm with fairness in mind.
